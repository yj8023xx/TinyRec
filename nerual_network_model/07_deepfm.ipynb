{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM\n",
    "\n",
    "<img src=\"../../img/deep_fm.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_id': 'movielens',\n",
       " 'num_fields': 26,\n",
       " 'feature_specs': {'movieId': {'source': 'item',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 935,\n",
       "   'index': 0},\n",
       "  'userId': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 22540,\n",
       "   'index': 1},\n",
       "  'rating': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 2},\n",
       "  'timestamp': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 3},\n",
       "  'releaseYear': {'source': 'item',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 4},\n",
       "  'movieGenre1': {'source': 'item',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 18,\n",
       "   'index': 5},\n",
       "  'movieGenre2': {'source': 'item',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 18,\n",
       "   'index': 6},\n",
       "  'movieGenre3': {'source': 'item',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 15,\n",
       "   'index': 7},\n",
       "  'movieRatingCount': {'source': 'item',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 8},\n",
       "  'movieAvgRating': {'source': 'item',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 9},\n",
       "  'movieRatingStddev': {'source': 'item',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 10},\n",
       "  'userRatedMovie1': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 882,\n",
       "   'index': 11},\n",
       "  'userRatedMovie2': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 874,\n",
       "   'index': 12},\n",
       "  'userRatedMovie3': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 877,\n",
       "   'index': 13},\n",
       "  'userRatedMovie4': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 871,\n",
       "   'index': 14},\n",
       "  'userRatedMovie5': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 864,\n",
       "   'index': 15},\n",
       "  'userRatingCount': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 16},\n",
       "  'userAvgReleaseYear': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 17},\n",
       "  'userReleaseYearStddev': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 18},\n",
       "  'userAvgRating': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 19},\n",
       "  'userRatingStddev': {'source': 'user',\n",
       "   'type': 'numerical',\n",
       "   'vocab_size': 1,\n",
       "   'index': 20},\n",
       "  'userGenre1': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 19,\n",
       "   'index': 21},\n",
       "  'userGenre2': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 19,\n",
       "   'index': 22},\n",
       "  'userGenre3': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 19,\n",
       "   'index': 23},\n",
       "  'userGenre4': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 19,\n",
       "   'index': 24},\n",
       "  'userGenre5': {'source': 'user',\n",
       "   'type': 'categorical',\n",
       "   'vocab_size': 19,\n",
       "   'index': 25}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/movielens/feature_map.json') as obj:\n",
    "    feature_map = json.load(obj)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, url):\n",
    "        self.df = pd.read_csv(url)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.df.iloc[idx, :-1].values.astype(np.float32), self.df.iloc[idx, -1].astype(np.float32)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovielensDataset('../data/movielens/data_for_train.csv')\n",
    "test_dataset = MovielensDataset('../data/movielens/data_for_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_map,\n",
    "                 embedding_dim=10,\n",
    "                 hidden_units=[256, 128, 64]):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_map = feature_map\n",
    "        # Embedding\n",
    "        self.embedding = nn.ModuleDict()\n",
    "        for feature, feature_spec in feature_map['feature_specs'].items():\n",
    "            if feature_spec['type'] == 'numerical':\n",
    "                self.embedding[feature] = nn.Linear(1, embedding_dim, bias=False)\n",
    "            elif feature_spec['type'] == 'categorical':\n",
    "                padding_idx = feature_spec.get('padding_idx', None)\n",
    "                self.embedding[feature] = nn.Embedding(feature_spec['vocab_size'],\n",
    "                                                       embedding_dim,\n",
    "                                                       padding_idx=padding_idx)\n",
    "        # FM\n",
    "        self.batch_norm = nn.BatchNorm1d(feature_map['num_fields'])\n",
    "        self.fm_layer = FM(feature_map['num_fields'])\n",
    "        # DNN\n",
    "        input_dim = feature_map['num_fields'] * embedding_dim\n",
    "        hidden_units = [input_dim] + hidden_units\n",
    "        hidden_layers = []\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            hidden_layers.append(nn.Linear(hidden_units[i], hidden_units[i + 1]))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        hidden_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "        self.dense_layer = nn.Sequential(*hidden_layers)\n",
    "        # Sigmoid\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        feature_emb_list = []\n",
    "        for feature, feature_spec in self.feature_map['feature_specs'].items():\n",
    "            if feature_spec['type'] == 'numerical':\n",
    "                raw_feature = X[:, feature_spec['index']].float().view(-1, 1)\n",
    "            elif feature_spec['type'] == 'categorical':\n",
    "                raw_feature = X[:, feature_spec['index']].long()\n",
    "            embedding_vec = self.embedding[feature](raw_feature)\n",
    "            feature_emb_list.append(embedding_vec)\n",
    "        feature_emb = torch.stack(feature_emb_list, dim=1)\n",
    "        out = self.fm_layer(self.batch_norm(X), feature_emb)\n",
    "        out += self.dense_layer(feature_emb.flatten(start_dim=1))\n",
    "        y_pred = self.output_activation(out).squeeze(1)\n",
    "        return y_pred\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FM, self).__init__()\n",
    "        self.lr_layer = nn.Linear(input_dim, 1)\n",
    "        self.product_layer = InnerProduct()\n",
    "\n",
    "    def forward(self, X, feature_emb):\n",
    "        out = self.lr_layer(X)\n",
    "        out += self.product_layer(feature_emb)\n",
    "        return out\n",
    "    \n",
    "class InnerProduct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InnerProduct, self).__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        sum_of_square = torch.sum(X, dim=1) ** 2 # sum then square\n",
    "        square_of_sum = torch.sum(X ** 2, dim=1) # square then sum\n",
    "        cross_term = sum_of_square - square_of_sum\n",
    "        return cross_term.sum(dim=-1, keepdim=True) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [300/1388] Loss: 30.3384\n",
      "Epoch [1/5], Step [600/1388] Loss: 25.1653\n",
      "Epoch [1/5], Step [900/1388] Loss: 26.0267\n",
      "Epoch [1/5], Step [1200/1388] Loss: 21.2643\n",
      "Epoch [2/5], Step [300/1388] Loss: 5.1713\n",
      "Epoch [2/5], Step [600/1388] Loss: 4.4765\n",
      "Epoch [2/5], Step [900/1388] Loss: 1.7232\n",
      "Epoch [2/5], Step [1200/1388] Loss: 0.0996\n",
      "Epoch [3/5], Step [300/1388] Loss: 1.7092\n",
      "Epoch [3/5], Step [600/1388] Loss: 3.6385\n",
      "Epoch [3/5], Step [900/1388] Loss: 1.6641\n",
      "Epoch [3/5], Step [1200/1388] Loss: 0.2319\n",
      "Epoch [4/5], Step [300/1388] Loss: 1.5725\n",
      "Epoch [4/5], Step [600/1388] Loss: 3.1596\n",
      "Epoch [4/5], Step [900/1388] Loss: 1.5932\n",
      "Epoch [4/5], Step [1200/1388] Loss: 1.5625\n",
      "Epoch [5/5], Step [300/1388] Loss: 1.6399\n",
      "Epoch [5/5], Step [600/1388] Loss: 0.0005\n",
      "Epoch [5/5], Step [900/1388] Loss: 3.1250\n",
      "Epoch [5/5], Step [1200/1388] Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(feature_map).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 300 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(\n",
    "                epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device).bool()\n",
    "        output = model(X)\n",
    "        y_pred = output > 0.5\n",
    "        total += y.shape[0]\n",
    "        correct += (y_pred == y).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {:.2f} %'.format(\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from ../data/trainingSamples.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown url type: '../data/trainingSamples.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a47ad5083c08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training samples path, change to your local path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m training_samples_file_path = tf.keras.utils.get_file(\n\u001b[0m\u001b[0;32m      3\u001b[0m     'trainingSamples.csv', '../data/trainingSamples.csv')\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Test samples path, change to your local path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m test_samples_file_path = tf.keras.utils.get_file('testSamples.csv',\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     80\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;31m# accept a URL or a Request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfullurl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    326\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                  method=None):\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mfull_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splittag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown url type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splithost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: '../data/trainingSamples.csv'"
     ]
    }
   ],
   "source": [
    "# Training samples path, change to your local path\n",
    "training_samples_file_path = tf.keras.utils.get_file(\n",
    "    'trainingSamples.csv', '../data/trainingSamples.csv')\n",
    "# Test samples path, change to your local path\n",
    "test_samples_file_path = tf.keras.utils.get_file('testSamples.csv',\n",
    "                                                 '../data/testSamples.csv')\n",
    "\n",
    "\n",
    "# load sample as tf dataset\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(file_path,\n",
    "                                                    batch_size=12,\n",
    "                                                    label_name='label',\n",
    "                                                    na_value=\"0\",\n",
    "                                                    num_epochs=1,\n",
    "                                                    ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# split as test dataset and training dataset\n",
    "train_data = get_dataset(training_samples_file_path)\n",
    "test_data = get_dataset(test_samples_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie id embedding feature\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(\n",
    "    key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "movie_ind_col = tf.feature_column.indicator_column(\n",
    "    movie_col)  # movid id indicator columns\n",
    "\n",
    "# user id embedding feature\n",
    "user_col = tf.feature_column.categorical_column_with_identity(\n",
    "    key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "user_ind_col = tf.feature_column.indicator_column(\n",
    "    user_col)  # user id indicator columns\n",
    "\n",
    "# genre features vocabulary\n",
    "genre_vocab = [\n",
    "    'Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy',\n",
    "    'Western', 'Documentary', 'Sci-Fi', 'Drama', 'Thriller', 'Crime',\n",
    "    'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical'\n",
    "]\n",
    "# user genre embedding feature\n",
    "user_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"userGenre1\", vocabulary_list=genre_vocab)\n",
    "user_genre_emb_col = tf.feature_column.embedding_column(user_genre_col, 10)\n",
    "user_genre_ind_col = tf.feature_column.indicator_column(\n",
    "    user_genre_col)  # user genre indicator columns\n",
    "# item genre embedding feature\n",
    "item_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"movieGenre1\", vocabulary_list=genre_vocab)\n",
    "item_genre_emb_col = tf.feature_column.embedding_column(item_genre_col, 10)\n",
    "item_genre_ind_col = tf.feature_column.indicator_column(\n",
    "    item_genre_col)  # item genre indicator columns\n",
    "\n",
    "# fm first-order term columns: without embedding and concatenate to the output layer directly\n",
    "fm_first_order_columns = [\n",
    "    movie_ind_col, user_ind_col, user_genre_ind_col, item_genre_ind_col\n",
    "]\n",
    "\n",
    "deep_feature_columns = [\n",
    "    tf.feature_column.numeric_column('releaseYear'),\n",
    "    tf.feature_column.numeric_column('movieRatingCount'),\n",
    "    tf.feature_column.numeric_column('movieAvgRating'),\n",
    "    tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "    tf.feature_column.numeric_column('userRatingCount'),\n",
    "    tf.feature_column.numeric_column('userAvgRating'),\n",
    "    tf.feature_column.numeric_column('userRatingStddev'), movie_emb_col,\n",
    "    user_emb_col\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:592: UserWarning: Input dict contained keys ['rating', 'timestamp', 'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4', 'userRatedMovie5', 'userAvgReleaseYear', 'userReleaseYearStddev'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7403/7403 [==============================] - 109s 14ms/step - loss: 0.8974 - accuracy: 0.6151 - auc: 0.6432 - auc_1: 0.6805\n",
      "Epoch 2/5\n",
      "7403/7403 [==============================] - 106s 14ms/step - loss: 0.5680 - accuracy: 0.7069 - auc: 0.7705 - auc_1: 0.7999\n",
      "Epoch 3/5\n",
      "7403/7403 [==============================] - 107s 14ms/step - loss: 0.5000 - accuracy: 0.7589 - auc: 0.8327 - auc_1: 0.8591\n",
      "Epoch 4/5\n",
      "7403/7403 [==============================] - 107s 14ms/step - loss: 0.4412 - accuracy: 0.7954 - auc: 0.8739 - auc_1: 0.8959\n",
      "Epoch 5/5\n",
      "7403/7403 [==============================] - 109s 15ms/step - loss: 0.3911 - accuracy: 0.8230 - auc: 0.9030 - auc_1: 0.9217\n",
      "1870/1870 [==============================] - 19s 10ms/step - loss: 0.7351 - accuracy: 0.6508 - auc: 0.6992 - auc_1: 0.7296\n",
      "\n",
      "\n",
      "Test Loss 0.7350901961326599, Test Accuracy 0.6508021354675293, Test ROC AUC 0.6991999745368958, Test PR AUC 0.7295635938644409\n"
     ]
    }
   ],
   "source": [
    "# define input for keras model\n",
    "inputs = {\n",
    "    'movieAvgRating':\n",
    "    tf.keras.layers.Input(name='movieAvgRating', shape=(), dtype='float32'),\n",
    "    'movieRatingStddev':\n",
    "    tf.keras.layers.Input(name='movieRatingStddev', shape=(), dtype='float32'),\n",
    "    'movieRatingCount':\n",
    "    tf.keras.layers.Input(name='movieRatingCount', shape=(), dtype='int32'),\n",
    "    'userAvgRating':\n",
    "    tf.keras.layers.Input(name='userAvgRating', shape=(), dtype='float32'),\n",
    "    'userRatingStddev':\n",
    "    tf.keras.layers.Input(name='userRatingStddev', shape=(), dtype='float32'),\n",
    "    'userRatingCount':\n",
    "    tf.keras.layers.Input(name='userRatingCount', shape=(), dtype='int32'),\n",
    "    'releaseYear':\n",
    "    tf.keras.layers.Input(name='releaseYear', shape=(), dtype='int32'),\n",
    "    'movieId':\n",
    "    tf.keras.layers.Input(name='movieId', shape=(), dtype='int32'),\n",
    "    'userId':\n",
    "    tf.keras.layers.Input(name='userId', shape=(), dtype='int32'),\n",
    "    'userRatedMovie1':\n",
    "    tf.keras.layers.Input(name='userRatedMovie1', shape=(), dtype='int32'),\n",
    "    'userGenre1':\n",
    "    tf.keras.layers.Input(name='userGenre1', shape=(), dtype='string'),\n",
    "    'userGenre2':\n",
    "    tf.keras.layers.Input(name='userGenre2', shape=(), dtype='string'),\n",
    "    'userGenre3':\n",
    "    tf.keras.layers.Input(name='userGenre3', shape=(), dtype='string'),\n",
    "    'userGenre4':\n",
    "    tf.keras.layers.Input(name='userGenre4', shape=(), dtype='string'),\n",
    "    'userGenre5':\n",
    "    tf.keras.layers.Input(name='userGenre5', shape=(), dtype='string'),\n",
    "    'movieGenre1':\n",
    "    tf.keras.layers.Input(name='movieGenre1', shape=(), dtype='string'),\n",
    "    'movieGenre2':\n",
    "    tf.keras.layers.Input(name='movieGenre2', shape=(), dtype='string'),\n",
    "    'movieGenre3':\n",
    "    tf.keras.layers.Input(name='movieGenre3', shape=(), dtype='string'),\n",
    "}\n",
    "\n",
    "item_emb_layer = tf.keras.layers.DenseFeatures([movie_emb_col])(inputs)\n",
    "user_emb_layer = tf.keras.layers.DenseFeatures([user_emb_col])(inputs)\n",
    "item_genre_emb_layer = tf.keras.layers.DenseFeatures([item_genre_emb_col\n",
    "                                                      ])(inputs)\n",
    "user_genre_emb_layer = tf.keras.layers.DenseFeatures([user_genre_emb_col\n",
    "                                                      ])(inputs)\n",
    "\n",
    "# The first-order term in the FM layer\n",
    "fm_first_order_layer = tf.keras.layers.DenseFeatures(fm_first_order_columns)(\n",
    "    inputs)\n",
    "\n",
    "# FM part, cross different categorical feature embeddings\n",
    "product_layer_item_user = tf.keras.layers.Dot(axes=1)(\n",
    "    [item_emb_layer, user_emb_layer])\n",
    "product_layer_item_genre_user_genre = tf.keras.layers.Dot(axes=1)(\n",
    "    [item_genre_emb_layer, user_genre_emb_layer])\n",
    "product_layer_item_genre_user = tf.keras.layers.Dot(axes=1)(\n",
    "    [item_genre_emb_layer, user_emb_layer])\n",
    "product_layer_user_genre_item = tf.keras.layers.Dot(axes=1)(\n",
    "    [item_emb_layer, user_genre_emb_layer])\n",
    "\n",
    "# deep part, MLP to generalize all input features\n",
    "deep = tf.keras.layers.DenseFeatures(deep_feature_columns)(inputs)\n",
    "deep = tf.keras.layers.Dense(64, activation='relu')(deep)\n",
    "deep = tf.keras.layers.Dense(64, activation='relu')(deep)\n",
    "\n",
    "# concatenate fm part and deep part\n",
    "concat_layer = tf.keras.layers.concatenate([\n",
    "    fm_first_order_layer, product_layer_item_user,\n",
    "    product_layer_item_genre_user_genre, product_layer_item_genre_user,\n",
    "    product_layer_user_genre_item, deep\n",
    "], axis=1)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(concat_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs, output_layer)\n",
    "# compile the model, set loss function, optimizer and evaluation metrics\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[\n",
    "                  'accuracy',\n",
    "                  tf.keras.metrics.AUC(curve='ROC'),\n",
    "                  tf.keras.metrics.AUC(curve='PR')\n",
    "              ])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_data, epochs=5)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(\n",
    "    test_data)\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}, Test ROC AUC {}, Test PR AUC {}'.\n",
    "      format(test_loss, test_accuracy, test_roc_auc, test_pr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted good rating: 38.51%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 14.12%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 26.67%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 83.84%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 70.61%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 96.47%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 48.75%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 91.97%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 89.56%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 57.76%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 18.68%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 34.84%  | Actual rating label:  Bad Rating\n"
     ]
    }
   ],
   "source": [
    "# print some predict results\n",
    "predictions = model.predict(test_data)\n",
    "for prediction, goodRating in zip(predictions[:12],\n",
    "                                  list(test_data)[0][1][:12]):\n",
    "    print(\"Predicted good rating: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual rating label: \",\n",
    "          (\"Good Rating\" if bool(goodRating) else \"Bad Rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFM_v2\n",
    "\n",
    "## 读取数据\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Diff with DeepFM:\n",
    "    1. separate categorical features from dense features when processing first order features and second order features\n",
    "    2. modify original fm part with a fully crossed fm part\n",
    "\"\"\"\n",
    "\n",
    "# Training samples path, change to your local path\n",
    "training_samples_file_path = tf.keras.utils.get_file(\n",
    "    'trainingSamples.csv', '../data/trainingSamples.csv')\n",
    "# Test samples path, change to your local path\n",
    "test_samples_file_path = tf.keras.utils.get_file('testSamples.csv',\n",
    "                                                 '../data/testSamples.csv')\n",
    "\n",
    "\n",
    "# load sample as tf dataset\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(file_path,\n",
    "                                                    batch_size=12,\n",
    "                                                    label_name='label',\n",
    "                                                    na_value=\"0\",\n",
    "                                                    num_epochs=1,\n",
    "                                                    ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# split as test dataset and training dataset\n",
    "train_data = get_dataset(training_samples_file_path)\n",
    "test_data = get_dataset(test_samples_file_path)\n",
    "\n",
    "## 特征工程\n",
    "\n",
    "# define input for keras model\n",
    "inputs = {\n",
    "    'movieAvgRating':\n",
    "    tf.keras.layers.Input(name='movieAvgRating', shape=(), dtype='float32'),\n",
    "    'movieRatingStddev':\n",
    "    tf.keras.layers.Input(name='movieRatingStddev', shape=(), dtype='float32'),\n",
    "    'movieRatingCount':\n",
    "    tf.keras.layers.Input(name='movieRatingCount', shape=(), dtype='int32'),\n",
    "    'userAvgRating':\n",
    "    tf.keras.layers.Input(name='userAvgRating', shape=(), dtype='float32'),\n",
    "    'userRatingStddev':\n",
    "    tf.keras.layers.Input(name='userRatingStddev', shape=(), dtype='float32'),\n",
    "    'userRatingCount':\n",
    "    tf.keras.layers.Input(name='userRatingCount', shape=(), dtype='int32'),\n",
    "    'releaseYear':\n",
    "    tf.keras.layers.Input(name='releaseYear', shape=(), dtype='int32'),\n",
    "    'movieId':\n",
    "    tf.keras.layers.Input(name='movieId', shape=(), dtype='int32'),\n",
    "    'userId':\n",
    "    tf.keras.layers.Input(name='userId', shape=(), dtype='int32'),\n",
    "    'userRatedMovie1':\n",
    "    tf.keras.layers.Input(name='userRatedMovie1', shape=(), dtype='int32'),\n",
    "    'userGenre1':\n",
    "    tf.keras.layers.Input(name='userGenre1', shape=(), dtype='string'),\n",
    "    'userGenre2':\n",
    "    tf.keras.layers.Input(name='userGenre2', shape=(), dtype='string'),\n",
    "    'userGenre3':\n",
    "    tf.keras.layers.Input(name='userGenre3', shape=(), dtype='string'),\n",
    "    'userGenre4':\n",
    "    tf.keras.layers.Input(name='userGenre4', shape=(), dtype='string'),\n",
    "    'userGenre5':\n",
    "    tf.keras.layers.Input(name='userGenre5', shape=(), dtype='string'),\n",
    "    'movieGenre1':\n",
    "    tf.keras.layers.Input(name='movieGenre1', shape=(), dtype='string'),\n",
    "    'movieGenre2':\n",
    "    tf.keras.layers.Input(name='movieGenre2', shape=(), dtype='string'),\n",
    "    'movieGenre3':\n",
    "    tf.keras.layers.Input(name='movieGenre3', shape=(), dtype='string'),\n",
    "}\n",
    "\n",
    "# movie id embedding feature\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(\n",
    "    key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "movie_ind_col = tf.feature_column.indicator_column(\n",
    "    movie_col)  # movid id indicator columns\n",
    "\n",
    "# user id embedding feature\n",
    "user_col = tf.feature_column.categorical_column_with_identity(\n",
    "    key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "user_ind_col = tf.feature_column.indicator_column(\n",
    "    user_col)  # user id indicator columns\n",
    "\n",
    "# genre features vocabulary\n",
    "genre_vocab = [\n",
    "    'Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy',\n",
    "    'Western', 'Documentary', 'Sci-Fi', 'Drama', 'Thriller', 'Crime',\n",
    "    'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical'\n",
    "]\n",
    "\n",
    "# user genre embedding feature\n",
    "user_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"userGenre1\", vocabulary_list=genre_vocab)\n",
    "user_genre_ind_col = tf.feature_column.indicator_column(user_genre_col)\n",
    "user_genre_emb_col = tf.feature_column.embedding_column(user_genre_col, 10)\n",
    "\n",
    "# item genre embedding feature\n",
    "item_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"movieGenre1\", vocabulary_list=genre_vocab)\n",
    "item_genre_ind_col = tf.feature_column.indicator_column(item_genre_col)\n",
    "item_genre_emb_col = tf.feature_column.embedding_column(item_genre_col, 10)\n",
    "\n",
    "# fm first-order categorical items\n",
    "cat_columns = [\n",
    "    movie_ind_col, user_ind_col, user_genre_ind_col, item_genre_ind_col\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    tf.feature_column.numeric_column('releaseYear'),\n",
    "    tf.feature_column.numeric_column('movieRatingCount'),\n",
    "    tf.feature_column.numeric_column('movieAvgRating'),\n",
    "    tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "    tf.feature_column.numeric_column('userRatingCount'),\n",
    "    tf.feature_column.numeric_column('userAvgRating'),\n",
    "    tf.feature_column.numeric_column('userRatingStddev')\n",
    "]\n",
    "\n",
    "## 模型训练与预测\n",
    "\n",
    "first_order_cat_feature = tf.keras.layers.DenseFeatures(cat_columns)(inputs)\n",
    "first_order_cat_feature = tf.keras.layers.Dense(\n",
    "    1, activation=None)(first_order_cat_feature)\n",
    "first_order_deep_feature = tf.keras.layers.DenseFeatures(deep_columns)(inputs)\n",
    "first_order_deep_feature = tf.keras.layers.Dense(\n",
    "    1, activation=None)(first_order_deep_feature)\n",
    "## first order feature\n",
    "\n",
    "first_order_feature = tf.keras.layers.Add()(\n",
    "    [first_order_cat_feature, first_order_deep_feature])\n",
    "\n",
    "second_order_cat_columns_emb = [\n",
    "    tf.keras.layers.DenseFeatures([item_genre_emb_col])(inputs),\n",
    "    tf.keras.layers.DenseFeatures([movie_emb_col])(inputs),\n",
    "    tf.keras.layers.DenseFeatures([user_genre_emb_col])(inputs),\n",
    "    tf.keras.layers.DenseFeatures([user_emb_col])(inputs)\n",
    "]\n",
    "\n",
    "second_order_cat_columns = []\n",
    "for feature_emb in second_order_cat_columns_emb:\n",
    "    feature = tf.keras.layers.Dense(64, activation=None)(feature_emb)\n",
    "    feature = tf.keras.layers.Reshape((-1, 64))(feature)\n",
    "    second_order_cat_columns.append(feature)\n",
    "\n",
    "second_order_deep_columns = tf.keras.layers.DenseFeatures(deep_columns)(inputs)\n",
    "second_order_deep_columns = tf.keras.layers.Dense(\n",
    "    64, activation=None)(second_order_deep_columns)\n",
    "second_order_deep_columns = tf.keras.layers.Reshape(\n",
    "    (-1, 64))(second_order_deep_columns)\n",
    "second_order_fm_feature = tf.keras.layers.Concatenate(\n",
    "    axis=1)(second_order_cat_columns + [second_order_deep_columns])\n",
    "\n",
    "## second_order_deep_feature\n",
    "deep_feature = tf.keras.layers.Flatten()(second_order_fm_feature)\n",
    "deep_feature = tf.keras.layers.Dense(32, activation='relu')(deep_feature)\n",
    "deep_feature = tf.keras.layers.Dense(16, activation='relu')(deep_feature)\n",
    "\n",
    "\n",
    "class ReduceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis, op='sum', **kwargs):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.op = op\n",
    "        assert self.op in ['sum', 'mean']\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        if self.op == 'sum':\n",
    "            return tf.reduce_sum(input, axis=self.axis)\n",
    "        elif self.op == 'mean':\n",
    "            return tf.reduce_mean(input, axis=self.axis)\n",
    "        return tf.reduce_sum(input, axis=self.axis)\n",
    "\n",
    "\n",
    "second_order_sum_feature = ReduceLayer(1)(second_order_fm_feature)\n",
    "second_order_sum_square_feature = tf.keras.layers.multiply(\n",
    "    [second_order_sum_feature, second_order_sum_feature])\n",
    "second_order_square_feature = tf.keras.layers.multiply(\n",
    "    [second_order_fm_feature, second_order_fm_feature])\n",
    "second_order_square_sum_feature = ReduceLayer(1)(second_order_square_feature)\n",
    "## second_order_fm_feature\n",
    "second_order_fm_feature = tf.keras.layers.subtract(\n",
    "    [second_order_sum_square_feature, second_order_square_sum_feature])\n",
    "\n",
    "concatenated_outputs = tf.keras.layers.Concatenate(axis=1)(\n",
    "    [first_order_feature, second_order_fm_feature, deep_feature])\n",
    "output_layer = tf.keras.layers.Dense(\n",
    "    1, activation='sigmoid')(concatenated_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs, output_layer)\n",
    "# compile the model, set loss function, optimizer and evaluation metrics\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[\n",
    "                  'accuracy',\n",
    "                  tf.keras.metrics.AUC(curve='ROC'),\n",
    "                  tf.keras.metrics.AUC(curve='PR')\n",
    "              ])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_data, epochs=5)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(test_data)\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}, Test ROC AUC {}, Test PR AUC {}'.\n",
    "      format(test_loss, test_accuracy, test_roc_auc, test_pr_auc))\n",
    "\n",
    "# print some predict results\n",
    "predictions = model.predict(test_data)\n",
    "for prediction, goodRating in zip(predictions[:12],\n",
    "                                  list(test_data)[0][1][:12]):\n",
    "    print(\"Predicted good rating: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual rating label: \",\n",
    "          (\"Good Rating\" if bool(goodRating) else \"Bad Rating\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
